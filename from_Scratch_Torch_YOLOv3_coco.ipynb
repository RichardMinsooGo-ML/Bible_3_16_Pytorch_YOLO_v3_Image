{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"from_Scratch_Torch_YOLOv3_coco.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsE7pg4+hak4rr+duEpqHG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Check GPU Type\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","    print('and then re-execute this cell.')\n","else:\n","    print(gpu_info)\n","    \n","\n","# Memory Space\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","    print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","    print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","    print('re-execute this cell.')\n","else:\n","    print('You are using a high-RAM runtime!')\n","    \n","\n","!pip install terminaltables\n","!pip install --upgrade --no-cache-dir gdown\n","\n","from IPython.display import clear_output \n","clear_output()\n","\n","# Clone from Github Repository\n","! git init .\n","! git remote add origin https://github.com/RichardMinsooGo-ML/Pytorch-Yolov3-Image.git\n","# ! git pull origin master\n","! git pull origin main\n","# Download Darknet Weights\n","# ! wget https://pjreddie.com/media/files/yolo-voc.weights \n","! wget https://pjreddie.com/media/files/yolov3-tiny.weights \n","! wget https://pjreddie.com/media/files/yolov3.weights\n","# ! wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights \n","# ! wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights\n","\n","import shutil\n","shutil.move(\"/content/yolov3-tiny.weights\", \"/content/checkpoints\")\n","shutil.move(\"/content/yolov3.weights\", \"/content/checkpoints\")\n","# shutil.move(\"/content/yolov4-tiny.weights\", \"/content/tmp\")\n","# shutil.move(\"/content/yolov4.weights\", \"/content/tmp\") \n","% rm -rf sample_data\n","\n","! wget http://images.cocodataset.org/zips/train2017.zip\n","! wget http://images.cocodataset.org/zips/val2017.zip\n","# ! wget http://images.cocodataset.org/zips/test2017.zip\n","# ! wget http://images.cocodataset.org/zips/unlabeled2017.zip\n","\n","\n","! unzip train2017.zip  -d dataset/COCO2017\n","! unzip val2017.zip  -d dataset/COCO2017\n","clear_output()\n","\n","# ! unzip test2017.zip\n","# clear_output()\n","\n","# ! unzip unlabeled2017.zip\n","# clear_output()\n","\n","! rm train2017.zip\n","! rm val2017.zip\n","# ! rm test2017.zip\n","# ! rm unlabeled2017.zip \n","\n","! wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","# wget http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip\n","# wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n","# wget http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip\n","\n","! unzip annotations_trainval2017.zip -d dataset/COCO2017\n","# ! unzip stuff_annotations_trainval2017.zip\n","# ! unzip image_info_test2017.zip\n","# ! unzip image_info_unlabeled2017.zip\n","\n","! rm annotations_trainval2017.zip\n","# ! rm stuff_annotations_trainval2017.zip\n","# ! rm image_info_test2017.zip\n","# ! rm image_info_unlabeled2017.zip\n","\n","# Build xml from json file\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import cv2\n","import json\n","\n","def write_to_xml(image_name, image_dict, data_folder, save_folder, xml_template='pascal_voc_template.xml'):\n","    \n","    \n","    # get bboxes\n","    bboxes = image_dict[image_name]\n","    \n","    # read xml file\n","    tree = ET.parse(xml_template)\n","    root = tree.getroot()    \n","    \n","    # modify\n","    folder = root.find('folder')\n","    folder.text = 'Annotations'\n","    \n","    fname = root.find('filename')\n","    fname.text = image_name.split('.')[0] \n","    \n","    src = root.find('source')\n","    database = src.find('database')\n","    database.text = 'COCO2017'\n","    \n","    \n","    # size\n","    img = cv2.imread(os.path.join(data_folder, image_name))\n","    h,w,d = img.shape\n","    \n","    size = root.find('size')\n","    width = size.find('width')\n","    width.text = str(w)\n","    height = size.find('height')\n","    height.text = str(h)\n","    depth = size.find('depth')\n","    depth.text = str(d)\n","    \n","    for box in bboxes:\n","        # append object\n","        obj = ET.SubElement(root, 'object')\n","        \n","        name = ET.SubElement(obj, 'name')\n","        name.text = box[0]\n","        \n","        pose = ET.SubElement(obj, 'pose')\n","        pose.text = 'Unspecified'\n","\n","        truncated = ET.SubElement(obj, 'truncated')\n","        truncated.text = str(0)\n","\n","        difficult = ET.SubElement(obj, 'difficult')\n","        difficult.text = str(0)\n","\n","        bndbox = ET.SubElement(obj, 'bndbox')\n","        \n","        xmin = ET.SubElement(bndbox, 'xmin')\n","        xmin.text = str(int(box[1]))\n","        \n","        ymin = ET.SubElement(bndbox, 'ymin')\n","        ymin.text = str(int(box[2]))\n","        \n","        xmax = ET.SubElement(bndbox, 'xmax')\n","        xmax.text = str(int(box[3]))\n","        \n","        ymax = ET.SubElement(bndbox, 'ymax')\n","        ymax.text = str(int(box[4]))\n","    \n","    # save .xml to anno_path\n","    anno_path = os.path.join(save_folder, image_name.split('.')[0] + '.xml')\n","    print(anno_path)\n","    tree.write(anno_path)\n","    \n","\n","# main routine\n","if __name__=='__main__':\n","    \n","    # read coco category list\n","    df = pd.read_csv('coco_categories.csv')\n","    df.set_index('id', inplace=True)\n","    \n","    # read annotations file\n","    # annotations_path = 'instances_val2017.json'\n","    # /content/annotations/instances_val2017.json\n","    \n","    annotations_path = '/content/dataset/COCO2017/annotations/instances_val2017.json'\n","\n","    # specify image locations\n","    image_folder = '/content/dataset/COCO2017/val2017'\n","    \n","    # specify savepath - where to save .xml files\n","    savepath = '/content/dataset/COCO2017/xml_val'\n","    if not os.path.exists(savepath):\n","        os.makedirs(savepath)\n","    \n","    # read in .json format\n","    with open(annotations_path,'rb') as file:\n","        doc = json.load(file)\n","        \n","    # get annotations\n","    annotations = doc['annotations']\n","    \n","    # iscrowd allowed? 1 for ok, else set to 0\n","    iscrowd_allowed = 1\n","    \n","    # initialize dict to store bboxes for each image\n","    image_dict = {}\n","    \n","    # loop through the annotations in the subset\n","    for anno in annotations:\n","        # get annotation for image name\n","        image_id = anno['image_id']\n","        image_name = '{0:012d}.jpg'.format(image_id)    \n","        \n","        # get category\n","        category = df.loc[anno['category_id']]['name']\n","        \n","        # add as a key to image_dict\n","        if not image_name in image_dict.keys():\n","            image_dict[image_name]=[]\n","        \n","        # append bounding boxes to it\n","        box = anno['bbox']\n","        # since bboxes = [xmin, ymin, width, height]:\n","        image_dict[image_name].append([category, box[0], box[1], box[0]+box[2], box[1]+box[3]])\n","        \n","    # generate .xml files\n","    for image_name in image_dict.keys():\n","        write_to_xml(image_name, image_dict, image_folder, savepath)\n","        print('generated for: ', image_name)\n","\n","    # read annotations file\n","    # annotations_path = 'instances_val2017.json'\n","    # /content/annotations/instances_train2017.json\n","    annotations_path = '/content/dataset/COCO2017/annotations/instances_train2017.json'\n","    \n","    # specify image locations\n","    image_folder = '/content/dataset/COCO2017/train2017'\n","    \n","    # specify savepath - where to save .xml files\n","    savepath = '/content/dataset/COCO2017/xml_train'\n","    if not os.path.exists(savepath):\n","        os.makedirs(savepath)\n","    \n","    # read in .json format\n","    with open(annotations_path,'rb') as file:\n","        doc = json.load(file)\n","        \n","    # get annotations\n","    annotations = doc['annotations']\n","    \n","    # iscrowd allowed? 1 for ok, else set to 0\n","    iscrowd_allowed = 1\n","    \n","    # initialize dict to store bboxes for each image\n","    image_dict = {}\n","    \n","    # loop through the annotations in the subset\n","    for anno in annotations:\n","        # get annotation for image name\n","        image_id = anno['image_id']\n","        image_name = '{0:012d}.jpg'.format(image_id)    \n","        \n","        # get category\n","        category = df.loc[anno['category_id']]['name']\n","        \n","        # add as a key to image_dict\n","        if not image_name in image_dict.keys():\n","            image_dict[image_name]=[]\n","        \n","        # append bounding boxes to it\n","        box = anno['bbox']\n","        # since bboxes = [xmin, ymin, width, height]:\n","        image_dict[image_name].append([category, box[0], box[1], box[0]+box[2], box[1]+box[3]])\n","        \n","    # generate .xml files\n","    for image_name in image_dict.keys():\n","        write_to_xml(image_name, image_dict, image_folder, savepath)\n","        print('generated for: ', image_name)  \n","    clear_output()\n","      \n","import os\n","\n","str_train = open(\"tmp_train.txt\", \"w\")\n","str_test  = open(\"tmp_val.txt\", \"w\")\n","\n","for path, subdirs, files in os.walk(r\"/content/dataset/COCO2017/xml_train\"):\n","    for filename in files:\n","        f = os.path.join(filename)\n","        f = os.path.splitext(f)[0]\n","        \n","        str_train.write(\"/content/dataset/COCO2017/images/\" + str(f) +\".jpg\"+ os.linesep)\n","        # str_train.write(str(f))\n","        \n","for path, subdirs, files in os.walk(r\"/content/dataset/COCO2017/xml_val\"):\n","    for filename in files:\n","        f = os.path.join(filename)\n","        f = os.path.splitext(f)[0]\n","        \n","        str_test.write(\"/content/dataset/COCO2017/images/\" + str(f) +\".jpg\"+ os.linesep)\n","        # str_test.write(str(f))\n","\n","\n","with open('tmp_train.txt') as infile, open('/content/dataset/COCO2017/train.txt', 'w') as outfile:\n","    for line in infile:\n","        if not line.strip(): continue  # skip the empty line\n","        outfile.write(line)  # non-empty line. Write it to output\n","with open('tmp_val.txt') as infile, open('/content/dataset/COCO2017/valid.txt', 'w') as outfile:\n","    for line in infile:\n","        if not line.strip(): continue  # skip the empty line\n","        outfile.write(line)  # non-empty line. Write it to output\n","\n","! rm tmp_train.txt\n","! rm tmp_val.txt       \n","\n","# VOC2012의 경우와 동일하게 사용하기 위해 iamges folder에 모든 데이터 저장\n","path = \"/content/dataset/COCO2017/images/\"\n","os.mkdir(path)\n","path = \"/content/dataset/COCO2017/labels/\"\n","os.mkdir(path)\n","path = \"/content/dataset/COCO2017/xml_files/\"\n","os.mkdir(path)\n","\n","# move files \n","import os\n","import shutil\n","\n","source_folder = r\"/content/dataset/COCO2017/val2017//\"\n","destination_folder = r\"/content/dataset/COCO2017/images//\"\n","\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","source_folder = r\"/content/dataset/COCO2017/train2017//\"\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","clear_output()\n","\n","\n","source_folder = r\"/content/dataset/COCO2017/xml_train//\"\n","destination_folder = r\"/content/dataset/COCO2017/xml_files//\"\n","\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","source_folder = r\"/content/dataset/COCO2017/xml_val//\"\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","clear_output()\n","\n","\n","% rm -rf /content/dataset/COCO2017/train2017\n","% rm -rf /content/dataset/COCO2017/val2017\n","% rm -rf /content/dataset/COCO2017/xml_train\n","% rm -rf /content/dataset/COCO2017/xml_val\n","\n","# Create VOC Dataset form\n","\"\"\"\n","출처: https://github.com/pjreddie/darknet/blob/master/scripts/voc_label.py\n","\n","Pascal VOC의 원본 annotation을 Yolo v3의 Object Detection label format으로 변환해주는 스크립트입니다.\n","이 파일을 VOCdevkit의 상위폴더로 옮긴 후, 해당 폴더에서 아래 명령을 실행하십시오.\n","    python voc_label.py\n","\n","출력되는 파일은 아래와 같습니다.\n","    2007_test.txt\n","    train.txt\n","    voc_classes.txt\n","2007_test.txt, train.txt 파일을 열어보면 이미지들의 위치가 절대경로로 적혀있습니다.\n","만약 상대경로로 바꿔주려면 VSCode에서 알맞게 변환해줍니다.\n","\"\"\"\n","\n","import os\n","import xml.etree.ElementTree as ET\n","import numpy as np\n","\n","# classes = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n","\n","classes = [ \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",  \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n","\n","def convert(size, box):\n","    dw = 1. / (size[0])\n","    dh = 1. / (size[1])\n","    x = (box[0] + box[1]) / 2.0 - 1\n","    y = (box[2] + box[3]) / 2.0 - 1\n","    w = box[1] - box[0]\n","    h = box[3] - box[2]\n","    x = x * dw\n","    w = w * dw\n","    y = y * dh\n","    h = h * dh\n","    return x, y, w, h\n","\n","def convert_annotation(image_id):\n","    in_file  = open('/content/dataset/COCO2017/xml_files/%s.xml'%(image_id))\n","    out_file = open('/content/dataset/COCO2017/labels/%s.txt'%(image_id), 'w')\n","    # in_file  = open('COCO2017/Annotations/%s.xml'%(image_id))\n","    # out_file = open('COCO2017/labels/%s.txt'%(image_id), 'w')\n","    tree = ET.parse(in_file)\n","    root = tree.getroot()\n","    size = root.find('size')\n","    w = int(size.find('width').text)\n","    h = int(size.find('height').text)\n","\n","    for obj in root.iter('object'):\n","        difficult = obj.find('difficult').text\n","        cls = obj.find('name').text\n","        if cls not in classes or int(difficult) == 1:\n","            continue\n","        cls_id = classes.index(cls)\n","        xmlbox = obj.find('bndbox')\n","        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),\n","             float(xmlbox.find('ymax').text))\n","        bb = convert((w, h), b)\n","        \n","        bb = np.around(bb, decimals=6)\n","        \n","        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n","\n","sets = [('train'), ('valid')]\n","\n","if __name__ == '__main__':\n","    wd = os.getcwd()\n","\n","    for image_set in sets:\n","        image_ids = open('/content/dataset/COCO2017/%s.txt' % (image_set)).read().strip().split()\n","        for image_id in image_ids:\n","            \n","            image_id = image_id.split(\"/\")\n","            image_id = image_id[5]\n","            image_id = image_id.split(\".\")\n","            image_id = image_id[0]\n","            \n","            # print(image_id)\n","            \n","            convert_annotation(image_id)\n","        # list_file.close()\n","          \n","clear_output()\n","# Train\n","from terminaltables import AsciiTable\n","\n","import os, sys, time, datetime, argparse\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","\n","import torch\n","import tqdm\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","\n","from torchvision import transforms\n","from utils.logger import *\n","from utils.utils import *\n","from utils.datasets import *\n","from utils.train_utils import *\n","from torch.autograd import Variable\n","import torch.optim as optim\n","from eval_mAP import evaluate_mAP\n","\n","from models.models import *\n","\n","\n","\"\"\" configuration json을 읽어들이는 class \"\"\"\n","class Config(dict): \n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","\n","    @classmethod\n","    def load(cls, file):\n","        with open(file, 'r') as f:\n","            config = json.loads(f.read())\n","            return Config(config)\n","\n","config = Config({\n","    \"data_config\"  : \"/content/config/coco.data\",\n","    \"model_def\"    : \"/content/config/yolov3.cfg\",\n","    \"trained_path\" : \"/content/checkpoints/yolov3.weights\",\n","    # \"saved_path\"   : \"/content/gdrive/MyDrive/Obj_detection_Torch_img/checkpoints/Yolo_V3_VOC.pth\",\n","    # \"trained_path\" : \"/content/checkpoints/Yolo_V3_VOC.pth\",\n","    \"saved_path\"   : \"/content/checkpoints/Yolo_V3_coco.pth\",\n","    \"working_dir\"  : './',\n","    \"num_epochs\"   : 3,\n","    \"batch_size\"   : 8,\n","    \"grad_accum\"   : 2,\n","    \"img_size\"     : 416,\n","    \"n_cpu\"        : 1\n","})\n","\n","print(config)\n","    \n","config.eval_interval = 2\n","config.multiscale_tr = True\n","config.ckpt_freq     = 2\n","config.iou_thres     = 0.5\n","config.conf_thres    = 0.5\n","config.nms_thres     = 0.5\n","\n","############## Dataset, logs, Checkpoints dir ######################\n","config.ckpt_dir = os.path.join(config.working_dir, 'checkpoints')\n","config.logs_dir = os.path.join(config.working_dir, 'logs')\n","\n","print(config)\n","\n","if not os.path.isdir(config.ckpt_dir):\n","    os.makedirs(config.ckpt_dir)\n","if not os.path.isdir(config.logs_dir):\n","    os.makedirs(config.logs_dir)\n","\n","############## Hardware configurations #############################    \n","config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Initiate model\n","model = Darknet(config.model_def).to(config.device)\n","# model.apply(weights_init_normal)\n","\n","# Get data configuration\n","data_config = parse_data_config(config.data_config)\n","train_path = data_config[\"train\"]\n","valid_path = data_config[\"valid\"]\n","\n","# If specified we start from checkpoint\n","\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","# torch.save(model.state_dict(), config.trained_path)\n","# sys.exit()\n","\n","class_names = load_classes(data_config[\"names\"])\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","metrics = [\n","    \"grid_size\",\n","    \"loss\",\n","    \"loss_x\",\n","    \"loss_y\",\n","    \"loss_w\",\n","    \"loss_h\",\n","    \"loss_obj\",\n","    \"loss_cls\",\n","    \"cls_acc\",\n","    \"recall50\",\n","    \"recall75\",\n","    \"precision\",\n","    \"conf_obj\",\n","    \"conf_noobj\",\n","]\n","\n","# learning rate scheduler config\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n","\n","# Create dataloader\n","# dataset = ListDataset(train_path, augment=True, multiscale=config.multiscale_tr)\n","# dataset = ListDataset(valid_path, augment=False, multiscale=False)\n","# dataset = ListDataset(train_path, augment=False, multiscale=False)\n","dataset = ListDataset(valid_path, augment=True, multiscale=config.multiscale_tr)\n","\n","train_dataloader = DataLoader(\n","    dataset,\n","    config.batch_size,\n","    shuffle=True,\n","    num_workers=config.n_cpu,\n","    pin_memory=True,\n","    collate_fn=dataset.collate_fn\n",")\n","\n","max_mAP = 0.0\n","for epoch in range(0, config.num_epochs, 1):\n","\n","    num_iters_per_epoch = len(train_dataloader)\n","\n","    # switch to train mode\n","    model.train()\n","    start_time = time.time()\n","\n","    epoch_loss = 0\n","    # Training        \n","    for batch_idx, batch_data in enumerate(tqdm.tqdm(train_dataloader)):\n","        \"\"\"\n","        print(batch_data[0])\n","        print(batch_data[1])\n","        print(batch_data[1].shape)\n","        print(batch_data[2])\n","\n","        imgs = batch_data[1]\n","\n","        from PIL import Image\n","        import numpy as np\n","\n","        w, h = imgs[0].shape[1], imgs[0].shape[2]\n","        src = imgs[0]\n","        # data = np.zeros((h, w, 3), dtype=np.uint8)\n","        # data[256, 256] = [255, 0, 0]\n","\n","        data = np.zeros((h, w, 3), dtype=np.uint8)\n","        data[:,:,0] = src[0,:,:]*255\n","        data[:,:,1] = src[1,:,:]*255\n","        data[:,:,2] = src[2,:,:]*255\n","        # img = Image.fromarray(data, 'RGB')\n","        img = Image.fromarray(data)\n","        img.save('my_img.png')\n","        img.show()\n","        \"\"\"\n","\n","        # data_time.update(time.time() - start_time)\n","        _, imgs, targets = batch_data\n","        global_step = num_iters_per_epoch * epoch + batch_idx + 1\n","\n","        targets = Variable(targets.to(config.device), requires_grad=False)\n","        imgs = Variable(imgs.to(config.device))\n","\n","        total_loss, outputs = model(imgs, targets)\n","\n","        epoch_loss += float(total_loss.item())\n","        # compute gradient and perform backpropagation\n","        total_loss.backward()\n","\n","        if global_step % config.grad_accum:\n","            # Accumulates gradient before each step\n","            optimizer.step()\n","            # Adjust learning rate\n","            lr_scheduler.step()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","        # ----------------\n","        #   Log progress\n","        # ----------------\n","        \"\"\"\n","        if (batch_idx+1)%int((len(train_dataloader)/4)) == 0:\n","\n","            log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % ((epoch+1), config.num_epochs, (batch_idx+1), len(train_dataloader))\n","\n","            metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n","\n","            # Log metrics at each YOLO layer\n","            for i, metric in enumerate(metrics):\n","                formats = {m: \"%.6f\" for m in metrics}\n","                formats[\"grid_size\"] = \"%2d\"\n","                formats[\"cls_acc\"] = \"%.2f%%\"\n","                row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n","                metric_table += [[metric, *row_metrics]]\n","\n","                # Tensorboard logging\n","                tensorboard_log = []\n","                for j, yolo in enumerate(model.yolo_layers):\n","                    for name, metric in yolo.metrics.items():\n","                        if name != \"grid_size\":\n","                            tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n","                tensorboard_log += [(\"loss\", total_loss.item())]\n","                # logger.list_of_scalars_summary(tensorboard_log, global_step)\n","\n","            log_str += AsciiTable(metric_table).table\n","            log_str += f\"\\nTotal loss {total_loss.item()}\"\n","\n","            # Determine approximate time left for epoch\n","            epoch_batches_left = len(train_dataloader) - (batch_idx + 1)\n","            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_idx + 1))\n","            log_str += f\"\\n---- ETA {time_left}\"\n","\n","            print(log_str)\n","\n","        # model.seen += imgs.size(0)\n","        \"\"\"\n","    \n","    crnt_epoch_loss = epoch_loss/num_iters_per_epoch\n","\n","    if (epoch+1)%3 == 0:\n","        torch.save(model.state_dict(), config.saved_path)\n","        print('Saved at {}'.format(config.saved_path))\n","    # global_epoch += 1\n","\n","    # print(\"Global_epoch :\",global_epoch, \"Current epoch loss : {:1.5f}\".format(crnt_epoch_loss),'Saved at {}'.format(config.trained_path))\n","    print(\"Current epoch loss : {:1.5f}\".format(crnt_epoch_loss))\n","\n","\n","# Evaulation        \n","#-------------------------------------------------------------------------------------\n","\n","# if (epoch+1)%8 == 0:\n","print(\"\\n---- Evaluating Model ----\")\n","# Evaluate the model on the validation set\n","precision, recall, AP, f1, ap_class = evaluate_mAP(model, valid_path, config,\n","    batch_size=4)\n","\n","val_metrics_dict = {\n","    'precision': precision.mean(),\n","    'recall': recall.mean(),\n","    'AP': AP.mean(),\n","    'f1': f1.mean(),\n","    'ap_class': ap_class.mean()\n","}\n","\n","# Print class APs and mAP\n","ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n","for i, c in enumerate(ap_class):\n","    ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n","print(AsciiTable(ap_table).table)\n","print(f\"---- mAP {AP.mean()}\")\n","\n","max_mAP = AP.mean()\n","#-------------------------------------------------------------------------------------\n","\"\"\"\n","# Save checkpoint\n","if (epoch+1) % config.ckpt_freq == 0:\n","    torch.save(model.state_dict(), config.trained_path)\n","    print('save a checkpoint at {}'.format(config.trained_path))\n","\"\"\"\n","\n","# Test images\n","from __future__ import division\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.ticker import NullLocator\n","from pathlib import Path\n","Path(\"output/images\").mkdir(parents=True, exist_ok=True)\n","import cv2\n","\n","config.image_folder = \"dataset/custom/images\"\n","config.class_path  = \"/content/dataset/COCO2017/coco.names\"\n","config.batch_size = 1\n","config.conf_thres = 0.8\n","config.nms_thres  = 0.4\n","\n","# Set up model\n","classes = load_classes(config.class_path)\n","# model.print_network()\n","print(\"\\n\\n\" + \"-*=\" * 30 + \"\\n\\n\")\n","assert os.path.isfile(config.trained_path), \"No file at {}\".format(config.trained_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","        \n","os.makedirs(\"output\", exist_ok=True)\n","# Eval mode\n","model.eval()\n","\n","dataloader = DataLoader(\n","    ImageFolder(config.image_folder, img_size=config.img_size),\n","    batch_size=config.batch_size,\n","    shuffle=False,\n","    num_workers=config.n_cpu,\n",")\n","\n","Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","\n","imgs = []  # Stores image paths\n","img_detections = []  # Stores detections for each image index\n","\n","print(\"\\nPerforming object detection:\")\n","start_time = time.time()\n","for batch_idx, (img_paths, input_imgs) in enumerate(dataloader):\n","    # Configure input\n","    input_imgs = Variable(input_imgs.type(Tensor))\n","\n","    # Get detections \n","    with torch.no_grad():\n","        detections = model(input_imgs)\n","        detections = non_max_suppression(detections, config.conf_thres, config.nms_thres)\n","\n","    # Log progress\n","    end_time = time.time()\n","    inference_time = datetime.timedelta(seconds=end_time - start_time)\n","    start_time = end_time\n","    print(\"\\t+ Batch %d, Inference Time: %s\" % (batch_idx, inference_time))\n","\n","    # Save image and detections\n","    imgs.extend(img_paths)\n","    img_detections.extend(detections)\n","\n","# Bounding-box colors\n","cmap = plt.get_cmap(\"tab20b\")\n","colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n","\n","print(\"\\nSaving images:\")\n","\n","# Iterate through images and save plot of detections\n","for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n","\n","    print(\"(%d) Image: '%s'\" % (img_i, path))\n","\n","    # Create plot\n","    img = np.array(Image.open(path))\n","    plt.figure()\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(img)\n","\n","    # Draw bounding boxes and labels of detections\n","    if detections is not None:\n","        # Rescale boxes to original image\n","        detections = rescale_boxes(detections, config.img_size, img.shape[:2])\n","        unique_labels = detections[:, -1].cpu().unique()\n","        n_cls_preds = len(unique_labels)\n","        bbox_colors = random.sample(colors, n_cls_preds)\n","        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n","\n","            print(\"\\t+ Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n","\n","            box_w = x2 - x1\n","            box_h = y2 - y1\n","\n","            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n","            # Create a Rectangle patch\n","            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=\"yellow\", facecolor=\"none\")\n","\n","            # Add the bbox to the plot\n","            ax.add_patch(bbox)\n","\n","            # Add label\n","            plt.text(x1,y1,s=classes[int(cls_pred)],color=\"white\",verticalalignment=\"top\",bbox={\"color\": 'C0', \"pad\": 0},\n","            )\n","\n","    # Save generated image with detections\n","    plt.axis(\"off\")\n","    plt.gca().xaxis.set_major_locator(NullLocator())\n","    plt.gca().yaxis.set_major_locator(NullLocator())\n","    filename = path.split(\"/\")[-1].split(\".\")[0]\n","\n","    plt.savefig(f\"output/pred_{filename}.jpg\", bbox_inches=\"tight\", pad_inches=0.0)\n","    plt.close()\n","\n","# mAP Calculation\n","import numpy as np\n","\n","config.batch_size  = 8\n","config.n_cpu  = 4\n","config.iou_thres  = 0.5\n","config.conf_thres = 0.5\n","config.nms_thres  = 0.5\n","\n","\n","# Get data configuration\n","data_config = parse_data_config(config.data_config)\n","valid_path = data_config[\"valid\"]\n","class_names = load_classes(data_config[\"names\"])\n","\n","# model.print_network()\n","print(\"\\n\" + \"___m__@@__m___\" * 10 + \"\\n\")\n","\n","print(config.trained_path)\n","\n","assert os.path.isfile(config.trained_path), \"No file at {}\".format(config.trained_path)\n","\n","# If specified we start from checkpoint\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","\n","print(valid_path)\n","print(\"\\nStart computing mAP...\\n\")\n","precision, recall, AP, f1, ap_class = evaluate_mAP(model, valid_path, config, batch_size = config.batch_size)\n","\n","print(\"\\nDone computing mAP...\\n\")\n","for idx, cls in enumerate(ap_class):\n","    print(\"\\t>>>\\t Class {} ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, \\\n","            class_names[cls][:3], precision[idx], recall[idx], AP[idx], f1[idx]))\n","\n","print(\"\\nmAP: {:.4}\\n\".format(AP.mean()))\n","\n","\n"],"metadata":{"id":"Be-jlt2pepvc"},"execution_count":null,"outputs":[]}]}