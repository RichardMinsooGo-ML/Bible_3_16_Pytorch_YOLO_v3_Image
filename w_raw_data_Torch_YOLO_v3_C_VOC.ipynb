{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOd/W8iEgmW5y6dWO2K51l1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lwqqFRwg9y7W"},"source":["## Check GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1666754841739,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"},"user_tz":-540},"id":"wUvbTuVkUMkF","outputId":"8d0c7ed1-895f-4321-c74c-744800eca6a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Oct 26 03:27:21 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# Check GPU Type\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","    print('and then re-execute this cell.')\n","else:\n","    print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"hOTMSpEDURD5"},"source":["# 2. Git Clone to root directory\n","At Colab, location is \"/content/\".\n","\n","Using `$ git clone https://github.com/lainyzine/git-clone.git` will load the Github repository with the name of the repository. With Colab, it can be convenient to work in the root directory. You can use the command to call the root directory as shown below.\n","\n","According to the Github repository, we can use `! git pull origin main` or `! Just use git pull origin master`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2012,"status":"ok","timestamp":1666755198520,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"},"user_tz":-540},"id":"ZaWwPFttIVXg","outputId":"a6543ae2-e6ab-4cd3-dede-ab16846d9d03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reinitialized existing Git repository in /content/.git/\n","fatal: remote origin already exists.\n","remote: Enumerating objects: 10, done.\u001b[K\n","remote: Counting objects: 100% (10/10), done.\u001b[K\n","remote: Compressing objects: 100% (7/7), done.\u001b[K\n","remote: Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (7/7), done.\n","From https://github.com/RichardMinsooGo-ML/Pytorch-Yolov3-Image\n"," * branch            main       -> FETCH_HEAD\n","   e9da88c..31bbf3f  main       -> origin/main\n","Updating e9da88c..31bbf3f\n","Fast-forward\n"," config/train_config.py |  73 \u001b[32m+++++++++++++++++++++++++\u001b[m\n"," train.py               | 141 \u001b[32m+++++++++++\u001b[m\u001b[31m--------------------------------------\u001b[m\n"," 2 files changed, 103 insertions(+), 111 deletions(-)\n"," create mode 100644 config/train_config.py\n"]}],"source":["# Clone from Github Repository\n","! git init .\n","! git remote add origin https://github.com/RichardMinsooGo-ML/Bible_3_16_Pytorch_YOLO_v3_Image.git\n","# ! git pull origin master\n","! git pull origin main"]},{"cell_type":"code","source":["# Memory Space\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","    print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","    print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","    print('re-execute this cell.')\n","else:\n","    print('You are using a high-RAM runtime!')\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiviAiDQr7s5","executionInfo":{"status":"ok","timestamp":1665905195175,"user_tz":-540,"elapsed":6,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"}},"outputId":"22df69d3-fe0a-4a92-8d6e-aeee1db822c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Oct 16 07:26:32 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   56C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n","menu, and then select High-RAM in the Runtime shape dropdown. Then, \n","re-execute this cell.\n"]}]},{"cell_type":"code","source":["!pip install terminaltables\n","!pip install --upgrade --no-cache-dir gdown\n","\n","from IPython.display import clear_output \n","clear_output()\n","\n","# Download Darknet Weights\n","# ! wget https://pjreddie.com/media/files/yolo-voc.weights \n","! wget https://pjreddie.com/media/files/yolov3-tiny.weights \n","! wget https://pjreddie.com/media/files/yolov3.weights\n","# ! wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights \n","# ! wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights\n","\n","import shutil\n","shutil.move(\"/content/yolov3-tiny.weights\", \"/content/checkpoints\")\n","shutil.move(\"/content/yolov3.weights\", \"/content/checkpoints\")\n","# shutil.move(\"/content/yolov4-tiny.weights\", \"/content/checkpoints\")\n","# shutil.move(\"/content/yolov4.weights\", \"/content/checkpoints\") \n"],"metadata":{"id":"4XzGi7B-snm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n","# ! mkdir dataset\n","!tar -xvf VOCtrainval_11-May-2012.tar -C /content/dataset\n","! rm /content/VOCtrainval_11-May-2012.tar\n","shutil.rmtree(\"sample_data\")\n","clear_output()\n","\n","\n","# train valid list file generation\n","import os\n","\n","str_train = open(\"tmp_train.txt\", \"w\")\n","str_test  = open(\"tmp_val.txt\", \"w\")\n","\n","count = 0\n","for path, subdirs, files in os.walk(r\"/content/dataset/VOCdevkit/VOC2012/Annotations\"):\n","    for filename in files:\n","        f = os.path.join(filename)\n","        f = os.path.splitext(f)[0]\n","        \n","        count += 1\n","        if count%5 == 0:\n","            str_test.write(\"/content/dataset/VOC2012/images/\" + str(f) +\".jpg\"+ os.linesep)\n","        else:\n","            str_train.write(\"/content/dataset/VOC2012/images/\" + str(f) +\".jpg\"+ os.linesep)\n","        # str_train.write(str(f))\n","        \n","with open('tmp_train.txt') as infile, open('/content/dataset/VOC2012/train.txt', 'w') as outfile:\n","    for line in infile:\n","        if not line.strip(): continue  # skip the empty line\n","        outfile.write(line)  # non-empty line. Write it to output\n","with open('tmp_val.txt') as infile, open('/content/dataset/VOC2012/valid.txt', 'w') as outfile:\n","    for line in infile:\n","        if not line.strip(): continue  # skip the empty line\n","        outfile.write(line)  # non-empty line. Write it to output\n","\n","! rm tmp_train.txt\n","! rm tmp_val.txt       \n","\n","path = \"/content/dataset/VOC2012/images/\"\n","os.mkdir(path)\n","path = \"/content/dataset/VOC2012/labels/\"\n","os.mkdir(path)\n","path = \"/content/dataset/VOC2012/xml_files/\"\n","os.mkdir(path)\n","\n","# move files \n","import os\n","import shutil\n","\n","source_folder = r\"/content/dataset/VOCdevkit/VOC2012/JPEGImages//\"\n","destination_folder = r\"/content/dataset/VOC2012/images//\"\n","\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","source_folder = r\"/content/dataset/VOCdevkit/VOC2012/Annotations//\"\n","destination_folder = r\"/content/dataset/VOC2012/xml_files//\"\n","# fetch all files\n","for file_name in os.listdir(source_folder):\n","    # construct full file path\n","    source = source_folder + file_name\n","    destination = destination_folder + file_name\n","    # move only files\n","    if os.path.isfile(source):\n","        shutil.move(source, destination)\n","        print('Moved:', file_name)\n","\n","clear_output()\n","\n","shutil.rmtree(\"/content/dataset/VOCdevkit\")\n","\n","# Create VOC Dataset form\n","\"\"\"\n","출처: https://github.com/pjreddie/darknet/blob/master/scripts/voc_label.py\n","\n","Pascal VOC의 원본 annotation을 Yolo v3의 Object Detection label format으로 변환해주는 스크립트입니다.\n","이 파일을 VOCdevkit의 상위폴더로 옮긴 후, 해당 폴더에서 아래 명령을 실행하십시오.\n","    python voc_label.py\n","\n","출력되는 파일은 아래와 같습니다.\n","    2007_test.txt\n","    train.txt\n","    voc_classes.txt\n","2007_test.txt, train.txt 파일을 열어보면 이미지들의 위치가 절대경로로 적혀있습니다.\n","만약 상대경로로 바꿔주려면 VSCode에서 알맞게 변환해줍니다.\n","\"\"\"\n","\n","import os\n","import xml.etree.ElementTree as ET\n","import numpy as np\n","\n","# classes = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n","\n","classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n","\n","def convert(size, box):\n","    dw = 1. / (size[0])\n","    dh = 1. / (size[1])\n","    x = (box[0] + box[1]) / 2.0 - 1\n","    y = (box[2] + box[3]) / 2.0 - 1\n","    w = box[1] - box[0]\n","    h = box[3] - box[2]\n","    x = x * dw\n","    w = w * dw\n","    y = y * dh\n","    h = h * dh\n","    return x, y, w, h\n","\n","def convert_annotation(image_id):\n","    in_file  = open('/content/dataset/VOC2012/xml_files/%s.xml'%(image_id))\n","    out_file = open('/content/dataset/VOC2012/labels/%s.txt'%(image_id), 'w')\n","    # in_file  = open('COCO2017/Annotations/%s.xml'%(image_id))\n","    # out_file = open('COCO2017/labels/%s.txt'%(image_id), 'w')\n","    tree = ET.parse(in_file)\n","    root = tree.getroot()\n","    size = root.find('size')\n","    w = int(size.find('width').text)\n","    h = int(size.find('height').text)\n","\n","    for obj in root.iter('object'):\n","        difficult = obj.find('difficult').text\n","        cls = obj.find('name').text\n","        if cls not in classes or int(difficult) == 1:\n","            continue\n","        cls_id = classes.index(cls)\n","        xmlbox = obj.find('bndbox')\n","        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),\n","             float(xmlbox.find('ymax').text))\n","        bb = convert((w, h), b)\n","        \n","        bb = np.around(bb, decimals=6)\n","        \n","        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n","\n","sets = [('train'), ('valid')]\n","\n","if __name__ == '__main__':\n","    wd = os.getcwd()\n","\n","    for image_set in sets:\n","        image_ids = open('/content/dataset/VOC2012/%s.txt' % (image_set)).read().strip().split()\n","        for image_id in image_ids:\n","            \n","            image_id = image_id.split(\"/\")\n","            image_id = image_id[5]\n","            image_id = image_id.split(\".\")\n","            image_id = image_id[0]\n","            \n","            # print(image_id)\n","            \n","            convert_annotation(image_id)\n","        # list_file.close()\n","          \n","clear_output()\n","\n","# Zip Dataset \n","# !zip -r /content/file.zip /content/dataset/VOC2012/labels\n"],"metadata":{"id":"-wMQxmAle5Il"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install terminaltables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SswqsO3htiML","executionInfo":{"status":"ok","timestamp":1665905629747,"user_tz":-540,"elapsed":4659,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"}},"outputId":"e58d3220-397b-41b0-da07-7b39ad81d404"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (3.1.10)\n"]}]},{"cell_type":"code","source":["! python train.py  --num_epochs 3 --batch_size 8 --data_config config/VOC.data --model_def config/yolov3.cfg --trained_path checkpoints/yolov3.weights --save_path checkpoints/Yolo_V3_VOC.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prutocrVzYXq","executionInfo":{"status":"ok","timestamp":1665908263026,"user_tz":-540,"elapsed":1068798,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"}},"outputId":"e9f82458-7600-40a8-e1f2-b6535f3596bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(batch_size=4, checkpoint_freq=2, ckpt_dir='./checkpoints', conf_thres=0.5, data_config='config/VOC.data', evaluation_interval=2, gradient_accumulations=2, img_size=416, iou_thres=0.5, logs_dir='./logs', model_def='config/yolov3.cfg', multiscale_training=True, n_cpu=1, nms_thres=0.5, num_epochs=2, save_path='checkpoints/Yolo_V3_VOC.pth', trained_path='checkpoints/yolov3.weights', working_dir='./')\n","\n","___m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m___\n","\n","checkpoints/yolov3.weights\n","Darknet weight loaded!\n"," 40% 1382/3420 [03:15<05:38,  6.03it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004562.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 56% 1912/3420 [04:29<02:38,  9.50it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_000763.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 78% 2682/3420 [06:19<01:30,  8.16it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_008051.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 96% 3278/3420 [07:45<00:18,  7.88it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005953.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","100% 3420/3420 [08:03<00:00,  7.07it/s]\n","Current epoch loss : 67.68565 Saved at checkpoints/Yolo_V3_VOC.pth\n"," 17% 572/3420 [01:24<07:13,  6.57it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005953.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 21% 710/3420 [01:43<05:11,  8.71it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004562.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 66% 2242/3420 [05:15<02:39,  7.41it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_000763.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 91% 3120/3420 [07:21<00:50,  5.90it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_008051.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","100% 3420/3420 [08:02<00:00,  7.08it/s]\n","Current epoch loss : 64.87040 Saved at checkpoints/Yolo_V3_VOC.pth\n","\n","---- Evaluating Model ----\n","Detecting objects:  47% 404/855 [00:41<00:46,  9.79it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004172.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  68% 578/855 [00:59<00:29,  9.53it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005262.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  77% 659/855 [01:08<00:20,  9.59it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005145.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  82% 698/855 [01:12<00:16,  9.61it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_007355.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects: 100% 855/855 [01:28<00:00,  9.64it/s]\n","Computing AP: 100% 20/20 [00:00<00:00, 1496.60it/s]\n","+-------+-------------+---------+\n","| Index | Class name  | AP      |\n","+-------+-------------+---------+\n","| 0     | aeroplane   | 0.00000 |\n","| 1     | bicycle     | 0.00000 |\n","| 2     | bird        | 0.00000 |\n","| 3     | boat        | 0.00000 |\n","| 4     | bottle      | 0.00000 |\n","| 5     | bus         | 0.00000 |\n","| 6     | car         | 0.00000 |\n","| 7     | cat         | 0.00000 |\n","| 8     | chair       | 0.00000 |\n","| 9     | cow         | 0.00000 |\n","| 10    | diningtable | 0.00000 |\n","| 11    | dog         | 0.00000 |\n","| 12    | horse       | 0.00000 |\n","| 13    | motorbike   | 0.00000 |\n","| 14    | person      | 0.10974 |\n","| 15    | pottedplant | 0.00000 |\n","| 16    | sheep       | 0.00000 |\n","| 17    | sofa        | 0.00000 |\n","| 18    | train       | 0.00000 |\n","| 19    | tvmonitor   | 0.00000 |\n","+-------+-------------+---------+\n","---- mAP 0.005486779199065974\n"]}]},{"cell_type":"code","source":["! python train.py  --num_epochs 3 --batch_size 16 --data_config config/VOC.data --model_def config/yolov3.cfg --trained_path checkpoints/Yolo_V3_VOC.pth --save_path checkpoints/Yolo_V3_VOC.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxUhAaDUz79h","executionInfo":{"status":"ok","timestamp":1665911021801,"user_tz":-540,"elapsed":1050468,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"}},"outputId":"65820aa3-82bf-4249-b8b3-9caf31bd6b27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(batch_size=16, checkpoint_freq=2, ckpt_dir='./checkpoints', conf_thres=0.5, data_config='config/VOC.data', evaluation_interval=2, gradient_accumulations=2, img_size=416, iou_thres=0.5, logs_dir='./logs', model_def='config/yolov3.cfg', multiscale_training=True, n_cpu=1, nms_thres=0.5, num_epochs=3, save_path='checkpoints/Yolo_V3_VOC.pth', trained_path='checkpoints/Yolo_V3_VOC.pth', working_dir='./')\n","\n","___m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m______m__@@__m___\n","\n","checkpoints/Yolo_V3_VOC.pth\n","Trained pytorch weight loaded!\n","  7% 57/855 [00:24<05:57,  2.23it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_008051.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 30% 257/855 [01:38<02:36,  3.81it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_000763.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 60% 511/855 [03:04<01:56,  2.96it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005953.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 77% 661/855 [03:54<00:52,  3.71it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004562.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","100% 855/855 [05:03<00:00,  2.82it/s]\n","Current epoch loss : 9.16676 Saved at checkpoints/Yolo_V3_VOC.pth\n","  2% 13/855 [00:03<04:09,  3.37it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_008051.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 28% 240/855 [01:24<04:45,  2.15it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_000763.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 45% 386/855 [02:22<02:22,  3.29it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004562.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 46% 390/855 [02:23<02:16,  3.40it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005953.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","100% 855/855 [05:17<00:00,  2.69it/s]\n","Current epoch loss : 8.33828 Saved at checkpoints/Yolo_V3_VOC.pth\n"," 32% 270/855 [01:41<03:43,  2.62it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004562.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 62% 534/855 [03:20<01:29,  3.57it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005953.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 67% 569/855 [03:33<01:25,  3.35it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_000763.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n"," 76% 650/855 [04:04<00:52,  3.92it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_008051.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","100% 855/855 [05:25<00:00,  2.63it/s]\n","Current epoch loss : 8.33391 Saved at checkpoints/Yolo_V3_VOC.pth\n","\n","---- Evaluating Model ----\n","Detecting objects:  47% 404/855 [00:44<00:47,  9.50it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_004172.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  68% 578/855 [01:02<00:30,  9.20it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005262.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  77% 659/855 [01:11<00:20,  9.47it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_005145.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects:  82% 698/855 [01:15<00:16,  9.31it/s]/content/utils/datasets.py:107: UserWarning: loadtxt: Empty input file: \"/content/dataset/VOC2012/labels/2008_007355.txt\"\n","  boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","Detecting objects: 100% 855/855 [01:32<00:00,  9.22it/s]\n","Computing AP: 100% 20/20 [00:00<00:00, 375.78it/s]\n","+-------+-------------+---------+\n","| Index | Class name  | AP      |\n","+-------+-------------+---------+\n","| 0     | aeroplane   | 0.06395 |\n","| 1     | bicycle     | 0.00000 |\n","| 2     | bird        | 0.00000 |\n","| 3     | boat        | 0.00000 |\n","| 4     | bottle      | 0.00000 |\n","| 5     | bus         | 0.00000 |\n","| 6     | car         | 0.01191 |\n","| 7     | cat         | 0.00000 |\n","| 8     | chair       | 0.00000 |\n","| 9     | cow         | 0.00000 |\n","| 10    | diningtable | 0.00000 |\n","| 11    | dog         | 0.00000 |\n","| 12    | horse       | 0.00000 |\n","| 13    | motorbike   | 0.00000 |\n","| 14    | person      | 0.36516 |\n","| 15    | pottedplant | 0.00000 |\n","| 16    | sheep       | 0.00000 |\n","| 17    | sofa        | 0.00000 |\n","| 18    | train       | 0.00000 |\n","| 19    | tvmonitor   | 0.00000 |\n","+-------+-------------+---------+\n","---- mAP 0.022051233715655823\n"]}]},{"cell_type":"code","source":["# Train\n","from terminaltables import AsciiTable\n","\n","import os, sys, time, datetime, argparse\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","\n","import torch\n","import tqdm\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","\n","from torchvision import transforms\n","from utils.logger import *\n","from utils.utils import *\n","from utils.datasets import *\n","from utils.train_utils import *\n","from torch.autograd import Variable\n","import torch.optim as optim\n","from eval_mAP import evaluate_mAP\n","\n","from models.models import *\n","\n","\n","\"\"\" configuration json을 읽어들이는 class \"\"\"\n","class Config(dict): \n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","\n","    @classmethod\n","    def load(cls, file):\n","        with open(file, 'r') as f:\n","            config = json.loads(f.read())\n","            return Config(config)\n","\n","config = Config({\n","    \"data_config\"  : \"/content/config/VOC.data\",\n","    \"model_def\"    : \"/content/config/yolov3.cfg\",\n","    \"trained_path\" : \"/content/checkpoints/yolov3.weights\",\n","    # \"saved_path\"   : \"/content/gdrive/MyDrive/Obj_detection_Torch_img/checkpoints/Yolo_V3_VOC.pth\",\n","    # \"trained_path\" : \"/content/checkpoints/Yolo_V3_VOC.pth\",\n","    \"saved_path\"   : \"/content/checkpoints/Yolo_V3_VOC.pth\",\n","    \"working_dir\"  : './',\n","    \"num_epochs\"   : 3,\n","    \"batch_size\"   : 8,\n","    \"grad_accum\"   : 2,\n","    \"img_size\"     : 416,\n","    \"n_cpu\"        : 1\n","})\n","\n","print(config)\n","    \n","config.eval_interval = 2\n","config.multiscale_tr = True\n","config.ckpt_freq     = 2\n","config.iou_thres     = 0.5\n","config.conf_thres    = 0.5\n","config.nms_thres     = 0.5\n","\n","############## Dataset, logs, Checkpoints dir ######################\n","config.ckpt_dir = os.path.join(config.working_dir, 'checkpoints')\n","config.logs_dir = os.path.join(config.working_dir, 'logs')\n","\n","print(config)\n","\n","if not os.path.isdir(config.ckpt_dir):\n","    os.makedirs(config.ckpt_dir)\n","if not os.path.isdir(config.logs_dir):\n","    os.makedirs(config.logs_dir)\n","\n","############## Hardware configurations #############################    \n","config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Initiate model\n","model = Darknet(config.model_def).to(config.device)\n","# model.apply(weights_init_normal)\n","\n","# Get data configuration\n","data_config = parse_data_config(config.data_config)\n","train_path = data_config[\"train\"]\n","valid_path = data_config[\"valid\"]\n","\n","# If specified we start from checkpoint\n","\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","# torch.save(model.state_dict(), config.trained_path)\n","# sys.exit()\n","\n","class_names = load_classes(data_config[\"names\"])\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","metrics = [\n","    \"grid_size\",\n","    \"loss\",\n","    \"loss_x\",\n","    \"loss_y\",\n","    \"loss_w\",\n","    \"loss_h\",\n","    \"loss_obj\",\n","    \"loss_cls\",\n","    \"cls_acc\",\n","    \"recall50\",\n","    \"recall75\",\n","    \"precision\",\n","    \"conf_obj\",\n","    \"conf_noobj\",\n","]\n","\n","# learning rate scheduler config\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n","\n","# Create dataloader\n","# dataset = ListDataset(train_path, augment=True, multiscale=config.multiscale_tr)\n","# dataset = ListDataset(valid_path, augment=False, multiscale=False)\n","dataset = ListDataset(train_path, augment=False, multiscale=False)\n","# dataset = ListDataset(valid_path, augment=True, multiscale=config.multiscale_tr)\n","\n","train_dataloader = DataLoader(\n","    dataset,\n","    config.batch_size,\n","    shuffle=True,\n","    num_workers=config.n_cpu,\n","    pin_memory=True,\n","    collate_fn=dataset.collate_fn\n",")\n","\n","max_mAP = 0.0\n","for epoch in range(0, config.num_epochs, 1):\n","\n","    num_iters_per_epoch = len(train_dataloader)\n","\n","    # switch to train mode\n","    model.train()\n","    start_time = time.time()\n","\n","    epoch_loss = 0\n","    # Training        \n","    for batch_idx, batch_data in enumerate(tqdm.tqdm(train_dataloader)):\n","        \"\"\"\n","        print(batch_data[0])\n","        print(batch_data[1])\n","        print(batch_data[1].shape)\n","        print(batch_data[2])\n","\n","        imgs = batch_data[1]\n","\n","        from PIL import Image\n","        import numpy as np\n","\n","        w, h = imgs[0].shape[1], imgs[0].shape[2]\n","        src = imgs[0]\n","        # data = np.zeros((h, w, 3), dtype=np.uint8)\n","        # data[256, 256] = [255, 0, 0]\n","\n","        data = np.zeros((h, w, 3), dtype=np.uint8)\n","        data[:,:,0] = src[0,:,:]*255\n","        data[:,:,1] = src[1,:,:]*255\n","        data[:,:,2] = src[2,:,:]*255\n","        # img = Image.fromarray(data, 'RGB')\n","        img = Image.fromarray(data)\n","        img.save('my_img.png')\n","        img.show()\n","        \"\"\"\n","\n","        # data_time.update(time.time() - start_time)\n","        _, imgs, targets = batch_data\n","        global_step = num_iters_per_epoch * epoch + batch_idx + 1\n","\n","        targets = Variable(targets.to(config.device), requires_grad=False)\n","        imgs = Variable(imgs.to(config.device))\n","\n","        total_loss, outputs = model(imgs, targets)\n","\n","        epoch_loss += float(total_loss.item())\n","        # compute gradient and perform backpropagation\n","        total_loss.backward()\n","\n","        if global_step % config.grad_accum:\n","            # Accumulates gradient before each step\n","            optimizer.step()\n","            # Adjust learning rate\n","            lr_scheduler.step()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","        # ----------------\n","        #   Log progress\n","        # ----------------\n","        \"\"\"\n","        if (batch_idx+1)%int((len(train_dataloader)/4)) == 0:\n","\n","            log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % ((epoch+1), config.num_epochs, (batch_idx+1), len(train_dataloader))\n","\n","            metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n","\n","            # Log metrics at each YOLO layer\n","            for i, metric in enumerate(metrics):\n","                formats = {m: \"%.6f\" for m in metrics}\n","                formats[\"grid_size\"] = \"%2d\"\n","                formats[\"cls_acc\"] = \"%.2f%%\"\n","                row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n","                metric_table += [[metric, *row_metrics]]\n","\n","                # Tensorboard logging\n","                tensorboard_log = []\n","                for j, yolo in enumerate(model.yolo_layers):\n","                    for name, metric in yolo.metrics.items():\n","                        if name != \"grid_size\":\n","                            tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n","                tensorboard_log += [(\"loss\", total_loss.item())]\n","                # logger.list_of_scalars_summary(tensorboard_log, global_step)\n","\n","            log_str += AsciiTable(metric_table).table\n","            log_str += f\"\\nTotal loss {total_loss.item()}\"\n","\n","            # Determine approximate time left for epoch\n","            epoch_batches_left = len(train_dataloader) - (batch_idx + 1)\n","            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_idx + 1))\n","            log_str += f\"\\n---- ETA {time_left}\"\n","\n","            print(log_str)\n","\n","        # model.seen += imgs.size(0)\n","        \"\"\"\n","    \n","    crnt_epoch_loss = epoch_loss/num_iters_per_epoch\n","\n","    if (epoch+1)%3 == 0:\n","        torch.save(model.state_dict(), config.saved_path)\n","        print('Saved at {}'.format(config.saved_path))\n","    # global_epoch += 1\n","\n","    # print(\"Global_epoch :\",global_epoch, \"Current epoch loss : {:1.5f}\".format(crnt_epoch_loss),'Saved at {}'.format(config.trained_path))\n","    print(\"Current epoch loss : {:1.5f}\".format(crnt_epoch_loss))\n","\n","\n","# Evaulation        \n","#-------------------------------------------------------------------------------------\n","\n","# if (epoch+1)%8 == 0:\n","print(\"\\n---- Evaluating Model ----\")\n","# Evaluate the model on the validation set\n","precision, recall, AP, f1, ap_class = evaluate_mAP(model, valid_path, config,\n","    batch_size=4)\n","\n","val_metrics_dict = {\n","    'precision': precision.mean(),\n","    'recall': recall.mean(),\n","    'AP': AP.mean(),\n","    'f1': f1.mean(),\n","    'ap_class': ap_class.mean()\n","}\n","\n","# Print class APs and mAP\n","ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n","for i, c in enumerate(ap_class):\n","    ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n","print(AsciiTable(ap_table).table)\n","print(f\"---- mAP {AP.mean()}\")\n","\n","max_mAP = AP.mean()\n","#-------------------------------------------------------------------------------------\n","\"\"\"\n","# Save checkpoint\n","if (epoch+1) % config.ckpt_freq == 0:\n","    torch.save(model.state_dict(), config.trained_path)\n","    print('save a checkpoint at {}'.format(config.trained_path))\n","\"\"\"\n","\n","# Test images\n","from __future__ import division\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.ticker import NullLocator\n","from pathlib import Path\n","Path(\"output/images\").mkdir(parents=True, exist_ok=True)\n","import cv2\n","\n","config.image_folder = \"dataset/custom/images\"\n","config.class_path  = \"/content/dataset/VOC2012/voc2012.names\"\n","config.batch_size = 1\n","config.conf_thres = 0.8\n","config.nms_thres  = 0.4\n","\n","# Set up model\n","classes = load_classes(config.class_path)\n","# model.print_network()\n","print(\"\\n\\n\" + \"-*=\" * 30 + \"\\n\\n\")\n","assert os.path.isfile(config.trained_path), \"No file at {}\".format(config.trained_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","        \n","os.makedirs(\"output\", exist_ok=True)\n","# Eval mode\n","model.eval()\n","\n","dataloader = DataLoader(\n","    ImageFolder(config.image_folder, img_size=config.img_size),\n","    batch_size=config.batch_size,\n","    shuffle=False,\n","    num_workers=config.n_cpu,\n",")\n","\n","Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n","\n","imgs = []  # Stores image paths\n","img_detections = []  # Stores detections for each image index\n","\n","print(\"\\nPerforming object detection:\")\n","start_time = time.time()\n","for batch_idx, (img_paths, input_imgs) in enumerate(dataloader):\n","    # Configure input\n","    input_imgs = Variable(input_imgs.type(Tensor))\n","\n","    # Get detections \n","    with torch.no_grad():\n","        detections = model(input_imgs)\n","        detections = non_max_suppression(detections, config.conf_thres, config.nms_thres)\n","\n","    # Log progress\n","    end_time = time.time()\n","    inference_time = datetime.timedelta(seconds=end_time - start_time)\n","    start_time = end_time\n","    print(\"\\t+ Batch %d, Inference Time: %s\" % (batch_idx, inference_time))\n","\n","    # Save image and detections\n","    imgs.extend(img_paths)\n","    img_detections.extend(detections)\n","\n","# Bounding-box colors\n","cmap = plt.get_cmap(\"tab20b\")\n","colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n","\n","print(\"\\nSaving images:\")\n","\n","# Iterate through images and save plot of detections\n","for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n","\n","    print(\"(%d) Image: '%s'\" % (img_i, path))\n","\n","    # Create plot\n","    img = np.array(Image.open(path))\n","    plt.figure()\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(img)\n","\n","    # Draw bounding boxes and labels of detections\n","    if detections is not None:\n","        # Rescale boxes to original image\n","        detections = rescale_boxes(detections, config.img_size, img.shape[:2])\n","        unique_labels = detections[:, -1].cpu().unique()\n","        n_cls_preds = len(unique_labels)\n","        bbox_colors = random.sample(colors, n_cls_preds)\n","        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n","\n","            print(\"\\t+ Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n","\n","            box_w = x2 - x1\n","            box_h = y2 - y1\n","\n","            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n","            # Create a Rectangle patch\n","            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=\"yellow\", facecolor=\"none\")\n","\n","            # Add the bbox to the plot\n","            ax.add_patch(bbox)\n","\n","            # Add label\n","            plt.text(x1,y1,s=classes[int(cls_pred)],color=\"white\",verticalalignment=\"top\",bbox={\"color\": 'C0', \"pad\": 0},\n","            )\n","\n","    # Save generated image with detections\n","    plt.axis(\"off\")\n","    plt.gca().xaxis.set_major_locator(NullLocator())\n","    plt.gca().yaxis.set_major_locator(NullLocator())\n","    filename = path.split(\"/\")[-1].split(\".\")[0]\n","\n","    plt.savefig(f\"output/pred_{filename}.jpg\", bbox_inches=\"tight\", pad_inches=0.0)\n","    plt.close()\n","\n","# mAP Calculation\n","import numpy as np\n","\n","config.batch_size  = 8\n","config.n_cpu  = 4\n","config.iou_thres  = 0.5\n","config.conf_thres = 0.5\n","config.nms_thres  = 0.5\n","\n","\n","# Get data configuration\n","data_config = parse_data_config(config.data_config)\n","valid_path = data_config[\"valid\"]\n","class_names = load_classes(data_config[\"names\"])\n","\n","# model.print_network()\n","print(\"\\n\" + \"___m__@@__m___\" * 10 + \"\\n\")\n","\n","print(config.trained_path)\n","\n","assert os.path.isfile(config.trained_path), \"No file at {}\".format(config.trained_path)\n","\n","# If specified we start from checkpoint\n","if config.trained_path:\n","    if config.trained_path.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(config.trained_path))\n","        print(\"Trained pytorch weight loaded!\")\n","    else:\n","        model.load_darknet_weights(config.trained_path)\n","        print(\"Darknet weight loaded!\")\n","\n","print(valid_path)\n","print(\"\\nStart computing mAP...\\n\")\n","precision, recall, AP, f1, ap_class = evaluate_mAP(model, valid_path, config, batch_size = config.batch_size)\n","\n","print(\"\\nDone computing mAP...\\n\")\n","for idx, cls in enumerate(ap_class):\n","    print(\"\\t>>>\\t Class {} ({}): precision = {:.4f}, recall = {:.4f}, AP = {:.4f}, f1: {:.4f}\".format(cls, \\\n","            class_names[cls][:3], precision[idx], recall[idx], AP[idx], f1[idx]))\n","\n","print(\"\\nmAP: {:.4}\\n\".format(AP.mean()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"EVh39M8JqLvy","executionInfo":{"status":"error","timestamp":1665907121567,"user_tz":-540,"elapsed":659,"user":{"displayName":"Richard Minsoo Go","userId":"09628967547080180869"}},"outputId":"b8dc74e3-5c4e-4539-fa37-39d5bec9fa4b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-7bfa3f1ece33>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from terminaltables import AsciiTable\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m import * only allowed at module level\n"]}]}]}